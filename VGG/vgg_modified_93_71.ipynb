{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEyE8sjbqsq6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "from keras.optimizers.legacy import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge85SgSxal84"
      },
      "outputs": [],
      "source": [
        "def VggModified(weight_decay, x_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olRWl99QK90C"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkYnZT_javWA"
      },
      "outputs": [],
      "source": [
        "vgg_m = VggModified(0.0007, [32, 32, 3], 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "cNbjF4QrytyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxfZTwnAq-og"
      },
      "outputs": [],
      "source": [
        "class cifar10vgg:\n",
        "    def __init__(self, model, lr, train=True):\n",
        "        self.num_classes = 10\n",
        "        self.weight_decay = 0.0005\n",
        "        self.x_shape = [32, 32, 3]\n",
        "\n",
        "        self.model = model\n",
        "        if train:\n",
        "            self.model = self.train(self.model, lr)\n",
        "        else:\n",
        "            self.model.load_weights('cifar10vgg.h5')\n",
        "\n",
        "    def normalize(self, X_train, X_test):\n",
        "        mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        X_train = (X_train - mean) / (std + 1e-7)\n",
        "        X_test = (X_test - mean) / (std + 1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "    def normalize_production(self, x):\n",
        "        mean = 120.707\n",
        "        std = 64.15\n",
        "        return (x - mean) / (std + 1e-7)\n",
        "\n",
        "    def predict(self, x, normalize=True, batch_size=50):\n",
        "        if normalize:\n",
        "            x = self.normalize_production(x)\n",
        "        return self.model.predict(x, batch_size)\n",
        "\n",
        "    def train(self, model, lr):\n",
        "        batch_size = 128\n",
        "        maxepoches = 250\n",
        "        learning_rate = lr\n",
        "        lr_decay = 1e-6\n",
        "        lr_drop = 20\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        sgd = SGD(learning_rate=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "        historytemp = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                epochs=maxepoches,\n",
        "                                validation_data=(x_test, y_test),\n",
        "                                callbacks=[reduce_lr, early_stopping],\n",
        "                                verbose=2)\n",
        "        model.save_weights('cifar10vgg.h5')\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGtesvN5a1T8",
        "outputId": "6bd66e44-f34a-4cc7-e0b5-c4c7d6e3ebbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "390/390 - 36s - loss: 6.5284 - accuracy: 0.3113 - val_loss: 4.7010 - val_accuracy: 0.1735 - lr: 0.1000 - 36s/epoch - 91ms/step\n",
            "Epoch 2/250\n",
            "390/390 - 32s - loss: 3.0746 - accuracy: 0.4821 - val_loss: 2.5065 - val_accuracy: 0.4909 - lr: 0.1000 - 32s/epoch - 81ms/step\n",
            "Epoch 3/250\n",
            "390/390 - 32s - loss: 1.9180 - accuracy: 0.5943 - val_loss: 1.8105 - val_accuracy: 0.5842 - lr: 0.1000 - 32s/epoch - 83ms/step\n",
            "Epoch 4/250\n",
            "390/390 - 33s - loss: 1.5172 - accuracy: 0.6717 - val_loss: 1.6329 - val_accuracy: 0.6365 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 5/250\n",
            "390/390 - 33s - loss: 1.4183 - accuracy: 0.7022 - val_loss: 1.3515 - val_accuracy: 0.7291 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 6/250\n",
            "390/390 - 33s - loss: 1.3926 - accuracy: 0.7187 - val_loss: 1.4133 - val_accuracy: 0.7204 - lr: 0.1000 - 33s/epoch - 86ms/step\n",
            "Epoch 7/250\n",
            "390/390 - 32s - loss: 1.3926 - accuracy: 0.7337 - val_loss: 1.3513 - val_accuracy: 0.7523 - lr: 0.1000 - 32s/epoch - 81ms/step\n",
            "Epoch 8/250\n",
            "390/390 - 33s - loss: 1.4115 - accuracy: 0.7363 - val_loss: 1.3828 - val_accuracy: 0.7474 - lr: 0.1000 - 33s/epoch - 85ms/step\n",
            "Epoch 9/250\n",
            "390/390 - 33s - loss: 1.4357 - accuracy: 0.7412 - val_loss: 1.5509 - val_accuracy: 0.7182 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 10/250\n",
            "390/390 - 32s - loss: 1.4499 - accuracy: 0.7460 - val_loss: 1.4415 - val_accuracy: 0.7548 - lr: 0.1000 - 32s/epoch - 81ms/step\n",
            "Epoch 11/250\n",
            "390/390 - 33s - loss: 1.4585 - accuracy: 0.7516 - val_loss: 1.3991 - val_accuracy: 0.7706 - lr: 0.1000 - 33s/epoch - 85ms/step\n",
            "Epoch 12/250\n",
            "390/390 - 32s - loss: 1.4694 - accuracy: 0.7526 - val_loss: 1.4469 - val_accuracy: 0.7597 - lr: 0.1000 - 32s/epoch - 83ms/step\n",
            "Epoch 13/250\n",
            "390/390 - 32s - loss: 1.4760 - accuracy: 0.7548 - val_loss: 1.4951 - val_accuracy: 0.7509 - lr: 0.1000 - 32s/epoch - 81ms/step\n",
            "Epoch 14/250\n",
            "390/390 - 33s - loss: 1.4912 - accuracy: 0.7525 - val_loss: 1.4860 - val_accuracy: 0.7597 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 15/250\n",
            "390/390 - 32s - loss: 1.4876 - accuracy: 0.7554 - val_loss: 1.4135 - val_accuracy: 0.7837 - lr: 0.1000 - 32s/epoch - 81ms/step\n",
            "Epoch 16/250\n",
            "390/390 - 34s - loss: 1.4881 - accuracy: 0.7595 - val_loss: 1.3923 - val_accuracy: 0.7940 - lr: 0.1000 - 34s/epoch - 87ms/step\n",
            "Epoch 17/250\n",
            "390/390 - 33s - loss: 1.4917 - accuracy: 0.7618 - val_loss: 1.4751 - val_accuracy: 0.7699 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 18/250\n",
            "390/390 - 33s - loss: 1.4892 - accuracy: 0.7619 - val_loss: 1.5286 - val_accuracy: 0.7549 - lr: 0.1000 - 33s/epoch - 84ms/step\n",
            "Epoch 19/250\n",
            "390/390 - 32s - loss: 1.4951 - accuracy: 0.7636 - val_loss: 1.5792 - val_accuracy: 0.7464 - lr: 0.1000 - 32s/epoch - 82ms/step\n",
            "Epoch 20/250\n",
            "390/390 - 34s - loss: 1.4882 - accuracy: 0.7651 - val_loss: 1.4088 - val_accuracy: 0.7872 - lr: 0.1000 - 34s/epoch - 86ms/step\n",
            "Epoch 21/250\n",
            "390/390 - 33s - loss: 1.2878 - accuracy: 0.8034 - val_loss: 1.1647 - val_accuracy: 0.8281 - lr: 0.0500 - 33s/epoch - 86ms/step\n",
            "Epoch 22/250\n",
            "390/390 - 34s - loss: 1.1826 - accuracy: 0.8099 - val_loss: 1.2292 - val_accuracy: 0.7936 - lr: 0.0500 - 34s/epoch - 86ms/step\n",
            "Epoch 23/250\n",
            "390/390 - 33s - loss: 1.1657 - accuracy: 0.8082 - val_loss: 1.1367 - val_accuracy: 0.8234 - lr: 0.0500 - 33s/epoch - 84ms/step\n",
            "Epoch 24/250\n",
            "390/390 - 32s - loss: 1.1737 - accuracy: 0.8083 - val_loss: 1.1429 - val_accuracy: 0.8237 - lr: 0.0500 - 32s/epoch - 81ms/step\n",
            "Epoch 25/250\n",
            "390/390 - 34s - loss: 1.1705 - accuracy: 0.8103 - val_loss: 1.1536 - val_accuracy: 0.8209 - lr: 0.0500 - 34s/epoch - 87ms/step\n",
            "Epoch 26/250\n",
            "390/390 - 34s - loss: 1.1782 - accuracy: 0.8096 - val_loss: 1.1358 - val_accuracy: 0.8283 - lr: 0.0500 - 34s/epoch - 88ms/step\n",
            "Epoch 27/250\n",
            "390/390 - 34s - loss: 1.1631 - accuracy: 0.8170 - val_loss: 1.2072 - val_accuracy: 0.8047 - lr: 0.0500 - 34s/epoch - 86ms/step\n",
            "Epoch 28/250\n",
            "390/390 - 34s - loss: 1.1812 - accuracy: 0.8137 - val_loss: 1.1323 - val_accuracy: 0.8260 - lr: 0.0500 - 34s/epoch - 86ms/step\n",
            "Epoch 29/250\n",
            "390/390 - 33s - loss: 1.1818 - accuracy: 0.8148 - val_loss: 1.1801 - val_accuracy: 0.8170 - lr: 0.0500 - 33s/epoch - 85ms/step\n",
            "Epoch 30/250\n",
            "390/390 - 35s - loss: 1.1915 - accuracy: 0.8154 - val_loss: 1.1756 - val_accuracy: 0.8219 - lr: 0.0500 - 35s/epoch - 90ms/step\n",
            "Epoch 31/250\n",
            "390/390 - 35s - loss: 1.1864 - accuracy: 0.8168 - val_loss: 1.1388 - val_accuracy: 0.8348 - lr: 0.0500 - 35s/epoch - 91ms/step\n",
            "Epoch 32/250\n",
            "390/390 - 33s - loss: 1.1943 - accuracy: 0.8155 - val_loss: 1.1995 - val_accuracy: 0.8099 - lr: 0.0500 - 33s/epoch - 86ms/step\n",
            "Epoch 33/250\n",
            "390/390 - 36s - loss: 1.1959 - accuracy: 0.8155 - val_loss: 1.2555 - val_accuracy: 0.7917 - lr: 0.0500 - 36s/epoch - 92ms/step\n",
            "Epoch 34/250\n",
            "390/390 - 34s - loss: 1.1993 - accuracy: 0.8161 - val_loss: 1.1739 - val_accuracy: 0.8239 - lr: 0.0500 - 34s/epoch - 88ms/step\n",
            "Epoch 35/250\n",
            "390/390 - 36s - loss: 1.1940 - accuracy: 0.8177 - val_loss: 1.3247 - val_accuracy: 0.7769 - lr: 0.0500 - 36s/epoch - 92ms/step\n",
            "Epoch 36/250\n",
            "390/390 - 35s - loss: 1.2015 - accuracy: 0.8202 - val_loss: 1.1676 - val_accuracy: 0.8268 - lr: 0.0500 - 35s/epoch - 89ms/step\n",
            "Epoch 37/250\n",
            "390/390 - 35s - loss: 1.1904 - accuracy: 0.8225 - val_loss: 1.2268 - val_accuracy: 0.8141 - lr: 0.0500 - 35s/epoch - 89ms/step\n",
            "Epoch 38/250\n",
            "390/390 - 34s - loss: 1.1997 - accuracy: 0.8201 - val_loss: 1.2359 - val_accuracy: 0.8115 - lr: 0.0500 - 34s/epoch - 88ms/step\n",
            "Epoch 39/250\n",
            "390/390 - 36s - loss: 1.1984 - accuracy: 0.8201 - val_loss: 1.1681 - val_accuracy: 0.8308 - lr: 0.0500 - 36s/epoch - 91ms/step\n",
            "Epoch 40/250\n",
            "390/390 - 35s - loss: 1.2040 - accuracy: 0.8212 - val_loss: 1.1381 - val_accuracy: 0.8420 - lr: 0.0500 - 35s/epoch - 89ms/step\n",
            "Epoch 41/250\n",
            "390/390 - 33s - loss: 1.0653 - accuracy: 0.8497 - val_loss: 0.9783 - val_accuracy: 0.8674 - lr: 0.0250 - 33s/epoch - 85ms/step\n",
            "Epoch 42/250\n",
            "390/390 - 33s - loss: 0.9725 - accuracy: 0.8612 - val_loss: 0.9292 - val_accuracy: 0.8721 - lr: 0.0250 - 33s/epoch - 84ms/step\n",
            "Epoch 43/250\n",
            "390/390 - 34s - loss: 0.9495 - accuracy: 0.8582 - val_loss: 0.9078 - val_accuracy: 0.8727 - lr: 0.0250 - 34s/epoch - 86ms/step\n",
            "Epoch 44/250\n",
            "390/390 - 34s - loss: 0.9328 - accuracy: 0.8593 - val_loss: 0.9421 - val_accuracy: 0.8587 - lr: 0.0250 - 34s/epoch - 87ms/step\n",
            "Epoch 45/250\n",
            "390/390 - 35s - loss: 0.9278 - accuracy: 0.8586 - val_loss: 0.9872 - val_accuracy: 0.8427 - lr: 0.0250 - 35s/epoch - 89ms/step\n",
            "Epoch 46/250\n",
            "390/390 - 34s - loss: 0.9236 - accuracy: 0.8604 - val_loss: 0.9711 - val_accuracy: 0.8475 - lr: 0.0250 - 34s/epoch - 87ms/step\n",
            "Epoch 47/250\n",
            "390/390 - 34s - loss: 0.9265 - accuracy: 0.8589 - val_loss: 0.9090 - val_accuracy: 0.8674 - lr: 0.0250 - 34s/epoch - 87ms/step\n",
            "Epoch 48/250\n",
            "390/390 - 33s - loss: 0.9291 - accuracy: 0.8597 - val_loss: 0.9626 - val_accuracy: 0.8515 - lr: 0.0250 - 33s/epoch - 83ms/step\n",
            "Epoch 49/250\n",
            "390/390 - 32s - loss: 0.9341 - accuracy: 0.8580 - val_loss: 0.9384 - val_accuracy: 0.8596 - lr: 0.0250 - 32s/epoch - 82ms/step\n",
            "Epoch 50/250\n",
            "390/390 - 34s - loss: 0.9275 - accuracy: 0.8617 - val_loss: 0.9660 - val_accuracy: 0.8493 - lr: 0.0250 - 34s/epoch - 88ms/step\n",
            "Epoch 51/250\n",
            "390/390 - 32s - loss: 0.9352 - accuracy: 0.8607 - val_loss: 0.9206 - val_accuracy: 0.8665 - lr: 0.0250 - 32s/epoch - 83ms/step\n",
            "Epoch 52/250\n",
            "390/390 - 34s - loss: 0.9319 - accuracy: 0.8629 - val_loss: 0.9748 - val_accuracy: 0.8463 - lr: 0.0250 - 34s/epoch - 88ms/step\n",
            "Epoch 53/250\n",
            "390/390 - 34s - loss: 0.9411 - accuracy: 0.8598 - val_loss: 0.9489 - val_accuracy: 0.8577 - lr: 0.0250 - 34s/epoch - 87ms/step\n",
            "Epoch 54/250\n",
            "390/390 - 32s - loss: 0.9417 - accuracy: 0.8612 - val_loss: 0.9577 - val_accuracy: 0.8578 - lr: 0.0250 - 32s/epoch - 82ms/step\n",
            "Epoch 55/250\n",
            "390/390 - 34s - loss: 0.9455 - accuracy: 0.8606 - val_loss: 0.9663 - val_accuracy: 0.8541 - lr: 0.0250 - 34s/epoch - 88ms/step\n",
            "Epoch 56/250\n",
            "390/390 - 33s - loss: 0.9444 - accuracy: 0.8598 - val_loss: 0.9435 - val_accuracy: 0.8623 - lr: 0.0250 - 33s/epoch - 85ms/step\n",
            "Epoch 57/250\n",
            "390/390 - 33s - loss: 0.9430 - accuracy: 0.8616 - val_loss: 0.9659 - val_accuracy: 0.8556 - lr: 0.0250 - 33s/epoch - 84ms/step\n",
            "Epoch 58/250\n",
            "390/390 - 32s - loss: 0.9506 - accuracy: 0.8600 - val_loss: 0.9052 - val_accuracy: 0.8763 - lr: 0.0250 - 32s/epoch - 82ms/step\n",
            "Epoch 59/250\n",
            "390/390 - 34s - loss: 0.9453 - accuracy: 0.8632 - val_loss: 0.9531 - val_accuracy: 0.8650 - lr: 0.0250 - 34s/epoch - 88ms/step\n",
            "Epoch 60/250\n",
            "390/390 - 33s - loss: 0.9449 - accuracy: 0.8627 - val_loss: 1.0560 - val_accuracy: 0.8323 - lr: 0.0250 - 33s/epoch - 85ms/step\n",
            "Epoch 61/250\n",
            "390/390 - 34s - loss: 0.8520 - accuracy: 0.8888 - val_loss: 0.8281 - val_accuracy: 0.8919 - lr: 0.0125 - 34s/epoch - 86ms/step\n",
            "Epoch 62/250\n",
            "390/390 - 35s - loss: 0.8009 - accuracy: 0.8965 - val_loss: 0.7915 - val_accuracy: 0.8941 - lr: 0.0125 - 35s/epoch - 89ms/step\n",
            "Epoch 63/250\n",
            "390/390 - 34s - loss: 0.7705 - accuracy: 0.8978 - val_loss: 0.7841 - val_accuracy: 0.8913 - lr: 0.0125 - 34s/epoch - 87ms/step\n",
            "Epoch 64/250\n",
            "390/390 - 34s - loss: 0.7573 - accuracy: 0.8966 - val_loss: 0.7800 - val_accuracy: 0.8907 - lr: 0.0125 - 34s/epoch - 87ms/step\n",
            "Epoch 65/250\n",
            "390/390 - 34s - loss: 0.7446 - accuracy: 0.8960 - val_loss: 0.8479 - val_accuracy: 0.8647 - lr: 0.0125 - 34s/epoch - 87ms/step\n",
            "Epoch 66/250\n",
            "390/390 - 34s - loss: 0.7294 - accuracy: 0.8988 - val_loss: 0.7597 - val_accuracy: 0.8904 - lr: 0.0125 - 34s/epoch - 86ms/step\n",
            "Epoch 67/250\n",
            "390/390 - 33s - loss: 0.7279 - accuracy: 0.8962 - val_loss: 0.7738 - val_accuracy: 0.8871 - lr: 0.0125 - 33s/epoch - 83ms/step\n",
            "Epoch 68/250\n",
            "390/390 - 35s - loss: 0.7312 - accuracy: 0.8941 - val_loss: 0.7518 - val_accuracy: 0.8888 - lr: 0.0125 - 35s/epoch - 89ms/step\n",
            "Epoch 69/250\n",
            "390/390 - 33s - loss: 0.7327 - accuracy: 0.8934 - val_loss: 0.7627 - val_accuracy: 0.8865 - lr: 0.0125 - 33s/epoch - 85ms/step\n",
            "Epoch 70/250\n",
            "390/390 - 33s - loss: 0.7225 - accuracy: 0.8971 - val_loss: 0.7435 - val_accuracy: 0.8919 - lr: 0.0125 - 33s/epoch - 85ms/step\n",
            "Epoch 71/250\n",
            "390/390 - 32s - loss: 0.7218 - accuracy: 0.8973 - val_loss: 0.7363 - val_accuracy: 0.8947 - lr: 0.0125 - 32s/epoch - 82ms/step\n",
            "Epoch 72/250\n",
            "390/390 - 34s - loss: 0.7274 - accuracy: 0.8956 - val_loss: 0.7537 - val_accuracy: 0.8868 - lr: 0.0125 - 34s/epoch - 87ms/step\n",
            "Epoch 73/250\n",
            "390/390 - 32s - loss: 0.7224 - accuracy: 0.8968 - val_loss: 0.7207 - val_accuracy: 0.9026 - lr: 0.0125 - 32s/epoch - 82ms/step\n",
            "Epoch 74/250\n",
            "390/390 - 34s - loss: 0.7242 - accuracy: 0.8970 - val_loss: 0.7506 - val_accuracy: 0.8907 - lr: 0.0125 - 34s/epoch - 87ms/step\n",
            "Epoch 75/250\n",
            "390/390 - 33s - loss: 0.7238 - accuracy: 0.8957 - val_loss: 0.7327 - val_accuracy: 0.9011 - lr: 0.0125 - 33s/epoch - 83ms/step\n",
            "Epoch 76/250\n",
            "390/390 - 32s - loss: 0.7245 - accuracy: 0.8975 - val_loss: 0.7421 - val_accuracy: 0.8940 - lr: 0.0125 - 32s/epoch - 82ms/step\n",
            "Epoch 77/250\n",
            "390/390 - 34s - loss: 0.7296 - accuracy: 0.8967 - val_loss: 0.7529 - val_accuracy: 0.8916 - lr: 0.0125 - 34s/epoch - 86ms/step\n",
            "Epoch 78/250\n",
            "390/390 - 32s - loss: 0.7327 - accuracy: 0.8951 - val_loss: 0.7735 - val_accuracy: 0.8875 - lr: 0.0125 - 32s/epoch - 82ms/step\n",
            "Epoch 79/250\n",
            "390/390 - 34s - loss: 0.7313 - accuracy: 0.8962 - val_loss: 0.7745 - val_accuracy: 0.8838 - lr: 0.0125 - 34s/epoch - 86ms/step\n",
            "Epoch 80/250\n",
            "390/390 - 33s - loss: 0.7282 - accuracy: 0.8979 - val_loss: 0.7346 - val_accuracy: 0.8994 - lr: 0.0125 - 33s/epoch - 84ms/step\n",
            "Epoch 81/250\n",
            "390/390 - 32s - loss: 0.6702 - accuracy: 0.9156 - val_loss: 0.6912 - val_accuracy: 0.9065 - lr: 0.0063 - 32s/epoch - 82ms/step\n",
            "Epoch 82/250\n",
            "390/390 - 34s - loss: 0.6306 - accuracy: 0.9239 - val_loss: 0.6634 - val_accuracy: 0.9162 - lr: 0.0063 - 34s/epoch - 86ms/step\n",
            "Epoch 83/250\n",
            "390/390 - 33s - loss: 0.6110 - accuracy: 0.9263 - val_loss: 0.6784 - val_accuracy: 0.9065 - lr: 0.0063 - 33s/epoch - 85ms/step\n",
            "Epoch 84/250\n",
            "390/390 - 33s - loss: 0.6003 - accuracy: 0.9268 - val_loss: 0.6584 - val_accuracy: 0.9109 - lr: 0.0063 - 33s/epoch - 85ms/step\n",
            "Epoch 85/250\n",
            "390/390 - 33s - loss: 0.5864 - accuracy: 0.9283 - val_loss: 0.6595 - val_accuracy: 0.9054 - lr: 0.0063 - 33s/epoch - 85ms/step\n",
            "Epoch 86/250\n",
            "390/390 - 33s - loss: 0.5775 - accuracy: 0.9280 - val_loss: 0.6475 - val_accuracy: 0.9086 - lr: 0.0063 - 33s/epoch - 85ms/step\n",
            "Epoch 87/250\n",
            "390/390 - 32s - loss: 0.5749 - accuracy: 0.9280 - val_loss: 0.6304 - val_accuracy: 0.9117 - lr: 0.0063 - 32s/epoch - 82ms/step\n",
            "Epoch 88/250\n",
            "390/390 - 33s - loss: 0.5689 - accuracy: 0.9283 - val_loss: 0.6554 - val_accuracy: 0.9028 - lr: 0.0063 - 33s/epoch - 85ms/step\n",
            "Epoch 89/250\n",
            "390/390 - 32s - loss: 0.5639 - accuracy: 0.9283 - val_loss: 0.6650 - val_accuracy: 0.9012 - lr: 0.0063 - 32s/epoch - 81ms/step\n",
            "Epoch 90/250\n",
            "390/390 - 33s - loss: 0.5595 - accuracy: 0.9275 - val_loss: 0.6553 - val_accuracy: 0.8989 - lr: 0.0063 - 33s/epoch - 84ms/step\n",
            "Epoch 91/250\n",
            "390/390 - 33s - loss: 0.5579 - accuracy: 0.9278 - val_loss: 0.6755 - val_accuracy: 0.8969 - lr: 0.0063 - 33s/epoch - 84ms/step\n",
            "Epoch 92/250\n",
            "390/390 - 33s - loss: 0.5567 - accuracy: 0.9278 - val_loss: 0.6174 - val_accuracy: 0.9111 - lr: 0.0063 - 33s/epoch - 84ms/step\n",
            "Epoch 93/250\n",
            "390/390 - 36s - loss: 0.5554 - accuracy: 0.9259 - val_loss: 0.6310 - val_accuracy: 0.9078 - lr: 0.0063 - 36s/epoch - 92ms/step\n",
            "Epoch 94/250\n",
            "390/390 - 35s - loss: 0.5515 - accuracy: 0.9272 - val_loss: 0.6455 - val_accuracy: 0.9009 - lr: 0.0063 - 35s/epoch - 91ms/step\n",
            "Epoch 95/250\n",
            "390/390 - 36s - loss: 0.5504 - accuracy: 0.9274 - val_loss: 0.6513 - val_accuracy: 0.8983 - lr: 0.0063 - 36s/epoch - 92ms/step\n",
            "Epoch 96/250\n",
            "390/390 - 34s - loss: 0.5479 - accuracy: 0.9269 - val_loss: 0.6573 - val_accuracy: 0.8989 - lr: 0.0063 - 34s/epoch - 87ms/step\n",
            "Epoch 97/250\n",
            "390/390 - 36s - loss: 0.5481 - accuracy: 0.9266 - val_loss: 0.6190 - val_accuracy: 0.9079 - lr: 0.0063 - 36s/epoch - 92ms/step\n",
            "Epoch 98/250\n",
            "390/390 - 35s - loss: 0.5456 - accuracy: 0.9279 - val_loss: 0.6111 - val_accuracy: 0.9128 - lr: 0.0063 - 35s/epoch - 90ms/step\n",
            "Epoch 99/250\n",
            "390/390 - 35s - loss: 0.5476 - accuracy: 0.9270 - val_loss: 0.6624 - val_accuracy: 0.8984 - lr: 0.0063 - 35s/epoch - 90ms/step\n",
            "Epoch 100/250\n",
            "390/390 - 35s - loss: 0.5480 - accuracy: 0.9269 - val_loss: 0.6235 - val_accuracy: 0.9071 - lr: 0.0063 - 35s/epoch - 89ms/step\n",
            "Epoch 101/250\n",
            "390/390 - 33s - loss: 0.5052 - accuracy: 0.9410 - val_loss: 0.5723 - val_accuracy: 0.9227 - lr: 0.0031 - 33s/epoch - 85ms/step\n",
            "Epoch 102/250\n",
            "390/390 - 35s - loss: 0.4839 - accuracy: 0.9460 - val_loss: 0.5983 - val_accuracy: 0.9124 - lr: 0.0031 - 35s/epoch - 91ms/step\n",
            "Epoch 103/250\n",
            "390/390 - 34s - loss: 0.4652 - accuracy: 0.9503 - val_loss: 0.5503 - val_accuracy: 0.9257 - lr: 0.0031 - 34s/epoch - 86ms/step\n",
            "Epoch 104/250\n",
            "390/390 - 32s - loss: 0.4598 - accuracy: 0.9496 - val_loss: 0.5641 - val_accuracy: 0.9217 - lr: 0.0031 - 32s/epoch - 83ms/step\n",
            "Epoch 105/250\n",
            "390/390 - 34s - loss: 0.4556 - accuracy: 0.9498 - val_loss: 0.5556 - val_accuracy: 0.9235 - lr: 0.0031 - 34s/epoch - 88ms/step\n",
            "Epoch 106/250\n",
            "390/390 - 33s - loss: 0.4468 - accuracy: 0.9517 - val_loss: 0.5560 - val_accuracy: 0.9232 - lr: 0.0031 - 33s/epoch - 84ms/step\n",
            "Epoch 107/250\n",
            "390/390 - 35s - loss: 0.4487 - accuracy: 0.9487 - val_loss: 0.5583 - val_accuracy: 0.9210 - lr: 0.0031 - 35s/epoch - 89ms/step\n",
            "Epoch 108/250\n",
            "390/390 - 38s - loss: 0.4388 - accuracy: 0.9529 - val_loss: 0.5556 - val_accuracy: 0.9227 - lr: 0.0031 - 38s/epoch - 97ms/step\n",
            "Epoch 109/250\n",
            "390/390 - 35s - loss: 0.4362 - accuracy: 0.9507 - val_loss: 0.5878 - val_accuracy: 0.9118 - lr: 0.0031 - 35s/epoch - 90ms/step\n",
            "Epoch 110/250\n",
            "390/390 - 35s - loss: 0.4297 - accuracy: 0.9519 - val_loss: 0.5248 - val_accuracy: 0.9253 - lr: 0.0031 - 35s/epoch - 89ms/step\n",
            "Epoch 111/250\n",
            "390/390 - 34s - loss: 0.4258 - accuracy: 0.9514 - val_loss: 0.5474 - val_accuracy: 0.9222 - lr: 0.0031 - 34s/epoch - 86ms/step\n",
            "Epoch 112/250\n",
            "390/390 - 34s - loss: 0.4255 - accuracy: 0.9516 - val_loss: 0.5421 - val_accuracy: 0.9251 - lr: 0.0031 - 34s/epoch - 88ms/step\n",
            "Epoch 113/250\n",
            "390/390 - 35s - loss: 0.4195 - accuracy: 0.9535 - val_loss: 0.5610 - val_accuracy: 0.9157 - lr: 0.0031 - 35s/epoch - 89ms/step\n",
            "Epoch 114/250\n",
            "390/390 - 36s - loss: 0.4173 - accuracy: 0.9535 - val_loss: 0.5394 - val_accuracy: 0.9202 - lr: 0.0031 - 36s/epoch - 93ms/step\n",
            "Epoch 115/250\n",
            "390/390 - 35s - loss: 0.4089 - accuracy: 0.9536 - val_loss: 0.5497 - val_accuracy: 0.9200 - lr: 0.0031 - 35s/epoch - 90ms/step\n",
            "Epoch 116/250\n",
            "390/390 - 35s - loss: 0.4192 - accuracy: 0.9504 - val_loss: 0.5602 - val_accuracy: 0.9140 - lr: 0.0031 - 35s/epoch - 91ms/step\n",
            "Epoch 117/250\n",
            "390/390 - 36s - loss: 0.4143 - accuracy: 0.9501 - val_loss: 0.5476 - val_accuracy: 0.9172 - lr: 0.0031 - 36s/epoch - 93ms/step\n",
            "Epoch 118/250\n",
            "390/390 - 37s - loss: 0.4133 - accuracy: 0.9505 - val_loss: 0.5514 - val_accuracy: 0.9137 - lr: 0.0031 - 37s/epoch - 95ms/step\n",
            "Epoch 119/250\n",
            "390/390 - 38s - loss: 0.4106 - accuracy: 0.9509 - val_loss: 0.5316 - val_accuracy: 0.9180 - lr: 0.0031 - 38s/epoch - 97ms/step\n",
            "Epoch 120/250\n",
            "390/390 - 37s - loss: 0.4067 - accuracy: 0.9528 - val_loss: 0.5218 - val_accuracy: 0.9240 - lr: 0.0031 - 37s/epoch - 96ms/step\n",
            "Epoch 121/250\n",
            "390/390 - 38s - loss: 0.3835 - accuracy: 0.9610 - val_loss: 0.5106 - val_accuracy: 0.9267 - lr: 0.0016 - 38s/epoch - 98ms/step\n",
            "Epoch 122/250\n",
            "390/390 - 36s - loss: 0.3698 - accuracy: 0.9642 - val_loss: 0.5189 - val_accuracy: 0.9236 - lr: 0.0016 - 36s/epoch - 92ms/step\n",
            "Epoch 123/250\n",
            "390/390 - 36s - loss: 0.3586 - accuracy: 0.9666 - val_loss: 0.5148 - val_accuracy: 0.9283 - lr: 0.0016 - 36s/epoch - 92ms/step\n",
            "Epoch 124/250\n",
            "390/390 - 36s - loss: 0.3551 - accuracy: 0.9668 - val_loss: 0.5198 - val_accuracy: 0.9254 - lr: 0.0016 - 36s/epoch - 91ms/step\n",
            "Epoch 125/250\n",
            "390/390 - 37s - loss: 0.3518 - accuracy: 0.9670 - val_loss: 0.5037 - val_accuracy: 0.9282 - lr: 0.0016 - 37s/epoch - 96ms/step\n",
            "Epoch 126/250\n",
            "390/390 - 36s - loss: 0.3466 - accuracy: 0.9678 - val_loss: 0.5009 - val_accuracy: 0.9310 - lr: 0.0016 - 36s/epoch - 92ms/step\n",
            "Epoch 127/250\n",
            "390/390 - 36s - loss: 0.3466 - accuracy: 0.9676 - val_loss: 0.4891 - val_accuracy: 0.9317 - lr: 0.0016 - 36s/epoch - 93ms/step\n",
            "Epoch 128/250\n",
            "390/390 - 36s - loss: 0.3395 - accuracy: 0.9693 - val_loss: 0.4902 - val_accuracy: 0.9304 - lr: 0.0016 - 36s/epoch - 93ms/step\n",
            "Epoch 129/250\n",
            "390/390 - 37s - loss: 0.3352 - accuracy: 0.9694 - val_loss: 0.4814 - val_accuracy: 0.9315 - lr: 0.0016 - 37s/epoch - 94ms/step\n",
            "Epoch 130/250\n",
            "390/390 - 37s - loss: 0.3335 - accuracy: 0.9689 - val_loss: 0.4919 - val_accuracy: 0.9281 - lr: 0.0016 - 37s/epoch - 96ms/step\n",
            "Epoch 131/250\n",
            "390/390 - 35s - loss: 0.3300 - accuracy: 0.9693 - val_loss: 0.4818 - val_accuracy: 0.9291 - lr: 0.0016 - 35s/epoch - 90ms/step\n",
            "Epoch 132/250\n",
            "390/390 - 35s - loss: 0.3271 - accuracy: 0.9694 - val_loss: 0.4925 - val_accuracy: 0.9279 - lr: 0.0016 - 35s/epoch - 89ms/step\n",
            "Epoch 133/250\n",
            "390/390 - 36s - loss: 0.3252 - accuracy: 0.9700 - val_loss: 0.4852 - val_accuracy: 0.9259 - lr: 0.0016 - 36s/epoch - 91ms/step\n",
            "Epoch 134/250\n",
            "390/390 - 38s - loss: 0.3243 - accuracy: 0.9690 - val_loss: 0.5114 - val_accuracy: 0.9231 - lr: 0.0016 - 38s/epoch - 96ms/step\n",
            "Epoch 135/250\n",
            "390/390 - 39s - loss: 0.3213 - accuracy: 0.9699 - val_loss: 0.4973 - val_accuracy: 0.9273 - lr: 0.0016 - 39s/epoch - 100ms/step\n",
            "Epoch 136/250\n",
            "390/390 - 35s - loss: 0.3225 - accuracy: 0.9688 - val_loss: 0.4880 - val_accuracy: 0.9246 - lr: 0.0016 - 35s/epoch - 90ms/step\n",
            "Epoch 137/250\n",
            "390/390 - 37s - loss: 0.3173 - accuracy: 0.9703 - val_loss: 0.4812 - val_accuracy: 0.9282 - lr: 0.0016 - 37s/epoch - 95ms/step\n",
            "Epoch 138/250\n",
            "390/390 - 36s - loss: 0.3142 - accuracy: 0.9705 - val_loss: 0.4794 - val_accuracy: 0.9293 - lr: 0.0016 - 36s/epoch - 93ms/step\n",
            "Epoch 139/250\n",
            "390/390 - 36s - loss: 0.3101 - accuracy: 0.9717 - val_loss: 0.4912 - val_accuracy: 0.9275 - lr: 0.0016 - 36s/epoch - 93ms/step\n",
            "Epoch 140/250\n",
            "390/390 - 37s - loss: 0.3084 - accuracy: 0.9708 - val_loss: 0.4934 - val_accuracy: 0.9273 - lr: 0.0016 - 37s/epoch - 96ms/step\n",
            "Epoch 141/250\n",
            "390/390 - 37s - loss: 0.2985 - accuracy: 0.9746 - val_loss: 0.4903 - val_accuracy: 0.9286 - lr: 7.8125e-04 - 37s/epoch - 94ms/step\n",
            "Epoch 142/250\n",
            "390/390 - 36s - loss: 0.2909 - accuracy: 0.9763 - val_loss: 0.4872 - val_accuracy: 0.9296 - lr: 7.8125e-04 - 36s/epoch - 93ms/step\n",
            "Epoch 143/250\n",
            "390/390 - 36s - loss: 0.2894 - accuracy: 0.9766 - val_loss: 0.4630 - val_accuracy: 0.9315 - lr: 7.8125e-04 - 36s/epoch - 91ms/step\n",
            "Epoch 144/250\n",
            "390/390 - 35s - loss: 0.2836 - accuracy: 0.9783 - val_loss: 0.4663 - val_accuracy: 0.9312 - lr: 7.8125e-04 - 35s/epoch - 89ms/step\n",
            "Epoch 145/250\n",
            "390/390 - 37s - loss: 0.2792 - accuracy: 0.9790 - val_loss: 0.4551 - val_accuracy: 0.9344 - lr: 7.8125e-04 - 37s/epoch - 94ms/step\n",
            "Epoch 146/250\n",
            "390/390 - 38s - loss: 0.2784 - accuracy: 0.9786 - val_loss: 0.4715 - val_accuracy: 0.9307 - lr: 7.8125e-04 - 38s/epoch - 99ms/step\n",
            "Epoch 147/250\n",
            "390/390 - 37s - loss: 0.2761 - accuracy: 0.9798 - val_loss: 0.4674 - val_accuracy: 0.9311 - lr: 7.8125e-04 - 37s/epoch - 96ms/step\n",
            "Epoch 148/250\n",
            "390/390 - 37s - loss: 0.2765 - accuracy: 0.9792 - val_loss: 0.4618 - val_accuracy: 0.9317 - lr: 7.8125e-04 - 37s/epoch - 94ms/step\n",
            "Epoch 149/250\n",
            "390/390 - 38s - loss: 0.2736 - accuracy: 0.9797 - val_loss: 0.4648 - val_accuracy: 0.9295 - lr: 7.8125e-04 - 38s/epoch - 98ms/step\n",
            "Epoch 150/250\n",
            "390/390 - 38s - loss: 0.2704 - accuracy: 0.9800 - val_loss: 0.4683 - val_accuracy: 0.9303 - lr: 7.8125e-04 - 38s/epoch - 97ms/step\n",
            "Epoch 151/250\n",
            "390/390 - 38s - loss: 0.2715 - accuracy: 0.9793 - val_loss: 0.4570 - val_accuracy: 0.9333 - lr: 7.8125e-04 - 38s/epoch - 98ms/step\n",
            "Epoch 152/250\n",
            "390/390 - 37s - loss: 0.2678 - accuracy: 0.9805 - val_loss: 0.4639 - val_accuracy: 0.9325 - lr: 7.8125e-04 - 37s/epoch - 94ms/step\n",
            "Epoch 153/250\n",
            "390/390 - 38s - loss: 0.2657 - accuracy: 0.9800 - val_loss: 0.4558 - val_accuracy: 0.9344 - lr: 7.8125e-04 - 38s/epoch - 97ms/step\n",
            "Epoch 154/250\n",
            "390/390 - 34s - loss: 0.2641 - accuracy: 0.9804 - val_loss: 0.4623 - val_accuracy: 0.9312 - lr: 7.8125e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 155/250\n",
            "390/390 - 37s - loss: 0.2628 - accuracy: 0.9806 - val_loss: 0.4514 - val_accuracy: 0.9344 - lr: 7.8125e-04 - 37s/epoch - 94ms/step\n",
            "Epoch 156/250\n",
            "390/390 - 37s - loss: 0.2636 - accuracy: 0.9804 - val_loss: 0.4548 - val_accuracy: 0.9317 - lr: 7.8125e-04 - 37s/epoch - 96ms/step\n",
            "Epoch 157/250\n",
            "390/390 - 36s - loss: 0.2587 - accuracy: 0.9809 - val_loss: 0.4541 - val_accuracy: 0.9331 - lr: 7.8125e-04 - 36s/epoch - 92ms/step\n",
            "Epoch 158/250\n",
            "390/390 - 37s - loss: 0.2589 - accuracy: 0.9809 - val_loss: 0.4470 - val_accuracy: 0.9345 - lr: 7.8125e-04 - 37s/epoch - 95ms/step\n",
            "Epoch 159/250\n",
            "390/390 - 37s - loss: 0.2581 - accuracy: 0.9810 - val_loss: 0.4596 - val_accuracy: 0.9299 - lr: 7.8125e-04 - 37s/epoch - 95ms/step\n",
            "Epoch 160/250\n",
            "390/390 - 36s - loss: 0.2510 - accuracy: 0.9824 - val_loss: 0.4620 - val_accuracy: 0.9287 - lr: 7.8125e-04 - 36s/epoch - 92ms/step\n",
            "Epoch 161/250\n",
            "390/390 - 38s - loss: 0.2532 - accuracy: 0.9806 - val_loss: 0.4465 - val_accuracy: 0.9343 - lr: 3.9063e-04 - 38s/epoch - 98ms/step\n",
            "Epoch 162/250\n",
            "390/390 - 36s - loss: 0.2463 - accuracy: 0.9844 - val_loss: 0.4471 - val_accuracy: 0.9342 - lr: 3.9063e-04 - 36s/epoch - 93ms/step\n",
            "Epoch 163/250\n",
            "390/390 - 35s - loss: 0.2465 - accuracy: 0.9838 - val_loss: 0.4437 - val_accuracy: 0.9342 - lr: 3.9063e-04 - 35s/epoch - 91ms/step\n",
            "Epoch 164/250\n",
            "390/390 - 37s - loss: 0.2433 - accuracy: 0.9839 - val_loss: 0.4456 - val_accuracy: 0.9350 - lr: 3.9063e-04 - 37s/epoch - 95ms/step\n",
            "Epoch 165/250\n",
            "390/390 - 36s - loss: 0.2413 - accuracy: 0.9846 - val_loss: 0.4450 - val_accuracy: 0.9359 - lr: 3.9063e-04 - 36s/epoch - 93ms/step\n",
            "Epoch 166/250\n",
            "390/390 - 37s - loss: 0.2404 - accuracy: 0.9843 - val_loss: 0.4426 - val_accuracy: 0.9343 - lr: 3.9063e-04 - 37s/epoch - 95ms/step\n",
            "Epoch 167/250\n",
            "390/390 - 37s - loss: 0.2421 - accuracy: 0.9838 - val_loss: 0.4397 - val_accuracy: 0.9344 - lr: 3.9063e-04 - 37s/epoch - 95ms/step\n",
            "Epoch 168/250\n",
            "390/390 - 35s - loss: 0.2424 - accuracy: 0.9834 - val_loss: 0.4426 - val_accuracy: 0.9345 - lr: 3.9063e-04 - 35s/epoch - 91ms/step\n",
            "Epoch 169/250\n",
            "390/390 - 36s - loss: 0.2361 - accuracy: 0.9860 - val_loss: 0.4467 - val_accuracy: 0.9352 - lr: 3.9063e-04 - 36s/epoch - 93ms/step\n",
            "Epoch 170/250\n",
            "390/390 - 36s - loss: 0.2367 - accuracy: 0.9853 - val_loss: 0.4432 - val_accuracy: 0.9349 - lr: 3.9063e-04 - 36s/epoch - 91ms/step\n",
            "Epoch 171/250\n",
            "390/390 - 35s - loss: 0.2361 - accuracy: 0.9851 - val_loss: 0.4384 - val_accuracy: 0.9364 - lr: 3.9063e-04 - 35s/epoch - 90ms/step\n",
            "Epoch 172/250\n",
            "390/390 - 35s - loss: 0.2338 - accuracy: 0.9855 - val_loss: 0.4415 - val_accuracy: 0.9352 - lr: 3.9063e-04 - 35s/epoch - 91ms/step\n",
            "Epoch 173/250\n",
            "390/390 - 35s - loss: 0.2353 - accuracy: 0.9851 - val_loss: 0.4378 - val_accuracy: 0.9362 - lr: 3.9063e-04 - 35s/epoch - 90ms/step\n",
            "Epoch 174/250\n",
            "390/390 - 35s - loss: 0.2322 - accuracy: 0.9863 - val_loss: 0.4406 - val_accuracy: 0.9351 - lr: 3.9063e-04 - 35s/epoch - 89ms/step\n",
            "Epoch 175/250\n",
            "390/390 - 34s - loss: 0.2316 - accuracy: 0.9855 - val_loss: 0.4434 - val_accuracy: 0.9338 - lr: 3.9063e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 176/250\n",
            "390/390 - 34s - loss: 0.2330 - accuracy: 0.9845 - val_loss: 0.4439 - val_accuracy: 0.9359 - lr: 3.9063e-04 - 34s/epoch - 86ms/step\n",
            "Epoch 177/250\n",
            "390/390 - 36s - loss: 0.2295 - accuracy: 0.9869 - val_loss: 0.4363 - val_accuracy: 0.9363 - lr: 3.9063e-04 - 36s/epoch - 92ms/step\n",
            "Epoch 178/250\n",
            "390/390 - 36s - loss: 0.2280 - accuracy: 0.9866 - val_loss: 0.4338 - val_accuracy: 0.9361 - lr: 3.9063e-04 - 36s/epoch - 92ms/step\n",
            "Epoch 179/250\n",
            "390/390 - 35s - loss: 0.2268 - accuracy: 0.9871 - val_loss: 0.4436 - val_accuracy: 0.9346 - lr: 3.9063e-04 - 35s/epoch - 91ms/step\n",
            "Epoch 180/250\n",
            "390/390 - 34s - loss: 0.2277 - accuracy: 0.9860 - val_loss: 0.4442 - val_accuracy: 0.9348 - lr: 3.9063e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 181/250\n",
            "390/390 - 34s - loss: 0.2279 - accuracy: 0.9861 - val_loss: 0.4441 - val_accuracy: 0.9345 - lr: 1.9531e-04 - 34s/epoch - 86ms/step\n",
            "Epoch 182/250\n",
            "390/390 - 33s - loss: 0.2253 - accuracy: 0.9867 - val_loss: 0.4397 - val_accuracy: 0.9356 - lr: 1.9531e-04 - 33s/epoch - 85ms/step\n",
            "Epoch 183/250\n",
            "390/390 - 34s - loss: 0.2234 - accuracy: 0.9875 - val_loss: 0.4407 - val_accuracy: 0.9357 - lr: 1.9531e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 184/250\n",
            "390/390 - 32s - loss: 0.2226 - accuracy: 0.9876 - val_loss: 0.4368 - val_accuracy: 0.9361 - lr: 1.9531e-04 - 32s/epoch - 82ms/step\n",
            "Epoch 185/250\n",
            "390/390 - 32s - loss: 0.2214 - accuracy: 0.9875 - val_loss: 0.4360 - val_accuracy: 0.9371 - lr: 1.9531e-04 - 32s/epoch - 83ms/step\n",
            "Epoch 186/250\n",
            "390/390 - 34s - loss: 0.2233 - accuracy: 0.9866 - val_loss: 0.4327 - val_accuracy: 0.9355 - lr: 1.9531e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 187/250\n",
            "390/390 - 33s - loss: 0.2200 - accuracy: 0.9879 - val_loss: 0.4351 - val_accuracy: 0.9363 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 188/250\n",
            "390/390 - 34s - loss: 0.2211 - accuracy: 0.9873 - val_loss: 0.4367 - val_accuracy: 0.9361 - lr: 1.9531e-04 - 34s/epoch - 88ms/step\n",
            "Epoch 189/250\n",
            "390/390 - 33s - loss: 0.2199 - accuracy: 0.9879 - val_loss: 0.4362 - val_accuracy: 0.9369 - lr: 1.9531e-04 - 33s/epoch - 83ms/step\n",
            "Epoch 190/250\n",
            "390/390 - 34s - loss: 0.2185 - accuracy: 0.9878 - val_loss: 0.4373 - val_accuracy: 0.9361 - lr: 1.9531e-04 - 34s/epoch - 87ms/step\n",
            "Epoch 191/250\n",
            "390/390 - 32s - loss: 0.2192 - accuracy: 0.9876 - val_loss: 0.4366 - val_accuracy: 0.9355 - lr: 1.9531e-04 - 32s/epoch - 82ms/step\n",
            "Epoch 192/250\n",
            "390/390 - 33s - loss: 0.2185 - accuracy: 0.9880 - val_loss: 0.4356 - val_accuracy: 0.9357 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 193/250\n",
            "390/390 - 32s - loss: 0.2188 - accuracy: 0.9875 - val_loss: 0.4354 - val_accuracy: 0.9351 - lr: 1.9531e-04 - 32s/epoch - 83ms/step\n",
            "Epoch 194/250\n",
            "390/390 - 33s - loss: 0.2182 - accuracy: 0.9879 - val_loss: 0.4340 - val_accuracy: 0.9363 - lr: 1.9531e-04 - 33s/epoch - 85ms/step\n",
            "Epoch 195/250\n",
            "390/390 - 33s - loss: 0.2178 - accuracy: 0.9875 - val_loss: 0.4333 - val_accuracy: 0.9360 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 196/250\n",
            "390/390 - 33s - loss: 0.2181 - accuracy: 0.9877 - val_loss: 0.4285 - val_accuracy: 0.9371 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 197/250\n",
            "390/390 - 33s - loss: 0.2163 - accuracy: 0.9884 - val_loss: 0.4330 - val_accuracy: 0.9366 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 198/250\n",
            "390/390 - 31s - loss: 0.2150 - accuracy: 0.9887 - val_loss: 0.4302 - val_accuracy: 0.9364 - lr: 1.9531e-04 - 31s/epoch - 81ms/step\n",
            "Epoch 199/250\n",
            "390/390 - 33s - loss: 0.2152 - accuracy: 0.9880 - val_loss: 0.4346 - val_accuracy: 0.9364 - lr: 1.9531e-04 - 33s/epoch - 84ms/step\n",
            "Epoch 200/250\n",
            "390/390 - 35s - loss: 0.2124 - accuracy: 0.9890 - val_loss: 0.4319 - val_accuracy: 0.9362 - lr: 1.9531e-04 - 35s/epoch - 89ms/step\n",
            "Epoch 201/250\n",
            "390/390 - 32s - loss: 0.2143 - accuracy: 0.9880 - val_loss: 0.4332 - val_accuracy: 0.9370 - lr: 9.7656e-05 - 32s/epoch - 81ms/step\n",
            "Epoch 202/250\n",
            "390/390 - 32s - loss: 0.2128 - accuracy: 0.9885 - val_loss: 0.4345 - val_accuracy: 0.9365 - lr: 9.7656e-05 - 32s/epoch - 83ms/step\n",
            "Epoch 203/250\n",
            "390/390 - 34s - loss: 0.2125 - accuracy: 0.9888 - val_loss: 0.4317 - val_accuracy: 0.9370 - lr: 9.7656e-05 - 34s/epoch - 86ms/step\n",
            "Epoch 204/250\n",
            "390/390 - 33s - loss: 0.2130 - accuracy: 0.9888 - val_loss: 0.4302 - val_accuracy: 0.9365 - lr: 9.7656e-05 - 33s/epoch - 85ms/step\n",
            "Epoch 205/250\n",
            "390/390 - 35s - loss: 0.2126 - accuracy: 0.9889 - val_loss: 0.4313 - val_accuracy: 0.9367 - lr: 9.7656e-05 - 35s/epoch - 90ms/step\n",
            "Epoch 206/250\n",
            "390/390 - 34s - loss: 0.2130 - accuracy: 0.9887 - val_loss: 0.4324 - val_accuracy: 0.9366 - lr: 9.7656e-05 - 34s/epoch - 88ms/step\n",
            "Epoch 207/250\n",
            "390/390 - 35s - loss: 0.2110 - accuracy: 0.9890 - val_loss: 0.4312 - val_accuracy: 0.9364 - lr: 9.7656e-05 - 35s/epoch - 90ms/step\n",
            "Epoch 208/250\n",
            "390/390 - 35s - loss: 0.2106 - accuracy: 0.9898 - val_loss: 0.4339 - val_accuracy: 0.9355 - lr: 9.7656e-05 - 35s/epoch - 89ms/step\n",
            "Epoch 209/250\n",
            "390/390 - 34s - loss: 0.2109 - accuracy: 0.9889 - val_loss: 0.4321 - val_accuracy: 0.9356 - lr: 9.7656e-05 - 34s/epoch - 86ms/step\n",
            "Epoch 210/250\n",
            "390/390 - 34s - loss: 0.2115 - accuracy: 0.9889 - val_loss: 0.4317 - val_accuracy: 0.9362 - lr: 9.7656e-05 - 34s/epoch - 87ms/step\n",
            "Epoch 211/250\n",
            "390/390 - 36s - loss: 0.2104 - accuracy: 0.9892 - val_loss: 0.4297 - val_accuracy: 0.9361 - lr: 9.7656e-05 - 36s/epoch - 93ms/step\n",
            "200/200 [==============================] - 1s 6ms/step\n",
            "the validation 0/1 loss is:  0.0629\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    model = cifar10vgg(vgg_m, 0.1)\n",
        "\n",
        "    predicted_x = model.predict(x_test)\n",
        "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\n",
        "\n",
        "    loss = sum(residuals)/len(residuals)\n",
        "    print(\"the validation 0/1 loss is: \",loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp6wZ6M4WaFJ",
        "outputId": "ad0a9356-f14d-405b-fd5f-ee00d91126af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9371\n"
          ]
        }
      ],
      "source": [
        "# Test accuracy calculation without using evaluate\n",
        "correct_predictions = np.sum(np.argmax(predicted_x, axis=1) == np.argmax(y_test, axis=1))\n",
        "test_accuracy = correct_predictions / len(y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the number of parameters\n",
        "total_params = vgg_m.count_params()\n",
        "print(f'Total parameters: {total_params}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd5iiE9PF8uh",
        "outputId": "0bc893d4-eb5f-47a8-a616-3d6a256f2904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 5749322\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}